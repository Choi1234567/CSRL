{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb6eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4993d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from agents.CSRL import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f13f4",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ffe36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_average(values, w=5):\n",
    "    '''\n",
    "    Function for smoothing an array using a sliding average\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    values (array-like): Values to smooth\n",
    "    w (int): Window of values to smooth over\n",
    "    '''\n",
    "    if w == 0:\n",
    "        return np.array(values)\n",
    "    else:\n",
    "        new_values = []\n",
    "        for i in range(len(values)):\n",
    "            new_values.append(np.mean(values[max(i - w, 0): min(i + w + 1, len(values))]))\n",
    "        return np.array(new_values)\n",
    "\n",
    "\n",
    "def plot_training(episode,rewards,losses):\n",
    "    \"\"\"Plot the training progress.\"\"\"\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('episode %s. rewards: %s' % (episode, np.mean(rewards[-10:])))\n",
    "    if len(rewards) > 40:\n",
    "        plt.plot(sliding_average(rewards, w=30))\n",
    "    else:\n",
    "        plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    if len(losses) > 40:\n",
    "        plt.plot(sliding_average(losses, w=30))\n",
    "    else:\n",
    "        plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d1c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_experiment(env, agent, train=True, log_r = False):\n",
    "    '''\n",
    "    Runs a single episode\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    env: environment, should contain the functions reset and step\n",
    "    agent: rl learning agent, should contain the functions get_action, add_experience_train, and end_episode\n",
    "    train (bool): whether to train the agent (set to False to evaluate)\n",
    "    log_r (bool): use the log of rewards to train\n",
    "    '''\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    all_rewards = []\n",
    "    all_rewards_unscaled = []\n",
    "    t = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        t+= 1\n",
    "        if train:\n",
    "            eps = None\n",
    "        else:\n",
    "            eps = EPS_MIN\n",
    "        action = agent.get_action(state, eps)\n",
    "        next_state, reward_unscaled, done, info = env.step(action)\n",
    "        \n",
    "        if log_r:\n",
    "            if reward_unscaled < 1:\n",
    "                reward = -10\n",
    "            else:\n",
    "                reward = np.log(reward_unscaled) - 10\n",
    "        else:\n",
    "            reward = reward_unscaled\n",
    "            \n",
    "        if train:\n",
    "            loss = agent.add_experience_train(state, action, reward, next_state, done)\n",
    "            total_loss += loss\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        all_rewards.append(reward)\n",
    "        all_rewards_unscaled.append(reward_unscaled)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward, total_loss, all_rewards, all_rewards_unscaled, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7984c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, n_episodes = 5000, log_r = False):\n",
    "    '''\n",
    "    Runs the episode and plots the progress\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    env: environment, should contain the functions reset and step\n",
    "    agent: rl learning agent, should contain the functions get_action, add_experience_train, and end_episode\n",
    "    n_episodes (int): number of episodes to run\n",
    "    log_r (bool): use the log of rewards to train\n",
    "    '''\n",
    "    rewards = []\n",
    "    rewards_unscaled = []\n",
    "    losses = []\n",
    "    for n in range(n_episodes):\n",
    "        total_reward, total_loss, all_rewards, all_rewards_unscaled, t = run_episode_experiment(env, agent, train=True, log_r = log_r)\n",
    "        removed_constraint = agent.end_episode(total_reward)\n",
    "        if removed_constraint is not None:\n",
    "            print(n, removed_constraint)\n",
    "        rewards.append(sum(all_rewards_unscaled))\n",
    "        losses.append(total_loss)\n",
    "        plot_training(n, np.array(rewards), np.array(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee4edbc",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Below contains the parameters for running the experiments.\n",
    "\n",
    "agent_params contains a dictionary of parameters for the base learning agents (ex. DDQN, Rainbow, etc) see the implementation file in the agents directory for detailed description of parameters.\n",
    "\n",
    "csrl_params contains a dictionary of parameters for the CSRL meta-selection agent, see the implementation file in the agents directory for details description of parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment_type (str):\n",
    "#ll: lunar lander\n",
    "#edu: education\n",
    "#hiv: hiv treatment\n",
    "\n",
    "experiment_type = 'edu' #Set the experiment type\n",
    "\n",
    "seed = 0 #Set the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc8ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "if experiment_type == 'll':\n",
    "    # Lunar Lander uses the pytorch implementations\n",
    "    import torch\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    from agents.cDDQN_pytorch import *\n",
    "    from agents.cRainbow_pytorch import *\n",
    "    \n",
    "    device = torch.device(f'cuda:{int(0)}' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "else:\n",
    "    #Education and HIV treatment use the tensorflow implementations\n",
    "    import tensorflow as tf\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    from agents.cDDQN_tf import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3007af",
   "metadata": {},
   "source": [
    "## Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_type == 'll':\n",
    "    \n",
    "    from environments_and_constraints.lunar_lander.utils import *\n",
    "    \n",
    "    folder = 'environments_and_constraints/lunar_lander/trained_agent_files/'\n",
    "    state_size = 8\n",
    "    action_size = 4\n",
    "    \n",
    "    #Make the environment\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    env.seed(seed)\n",
    "    \n",
    "    #Create the constraints\n",
    "    #Load in the \"human\" policies that we use as constraints\n",
    "    policies_dict = {key: DQNLoadedPolicy(state_size, action_size, filename = folder+save_model_paths[key], device = device) for key in policies_needed}\n",
    "    constraints_names = ['none', 'p0_1', 'p1_1', 'p2_1', 'p3_1', 'p4_1', 'p5_1', 'p6_1', 'p7_1', 'p01_1', 'p23_1', 'p45_1', 'p67_1']\n",
    "    constraint_func_list, epsilon_decay_list, more_constrained_constraints_list, less_constrained_constraints_list \\\n",
    "    = get_info(constraints_names, constraint_name_to_lists, epsilon_decays, more_constrained_constraints,\n",
    "                 less_constrained_constraints, policies_dict)\n",
    "    \n",
    "    #Set the params of the base learning agent\n",
    "    agent_params = {\n",
    "        'state_size': state_size,\n",
    "        'action_size': action_size,\n",
    "        'device': device,\n",
    "        'gamma': 0.99,\n",
    "        'max_buffer_size': int(1e5),\n",
    "        'batch_size': 64,\n",
    "        'lr': 1e-3,\n",
    "        'copy_param': 4,\n",
    "        'double': False,\n",
    "        'epsilon_init': 1,\n",
    "        'decay_epsilon_experience': True\n",
    "    }\n",
    "    \n",
    "    #Set the params of the CSRL metaselection agent\n",
    "    csrl_params = {\n",
    "        'constraint_conf_mult': 1,\n",
    "        'max_value': 600,\n",
    "        'offset': 0.5,\n",
    "        'policy_track_length': 20,\n",
    "        'loss_threshold': 0.1,\n",
    "    }\n",
    "    \n",
    "    #agent_type selects the learner to use:\n",
    "    agent_type = 'CSRL'\n",
    "    constraint_func = constraint_func_list[0]\n",
    "    epsilon_decay = epsilon_decay_list[0]\n",
    "    if agent_type == 'DQN':\n",
    "        agent_params['double'] = False\n",
    "        agent = DDQN_torch(constraint_func = constraint_func, epsilon_decay = epsilon_decay, **agent_params)\n",
    "    elif agent_type == 'DDQN':\n",
    "        agent_params['double'] = True\n",
    "        agent = DDQN_torch(constraint_func = constraint_func, epsilon_decay = epsilon_decay, **agent_params)\n",
    "    elif agent_type == 'Rainbow':\n",
    "        agent = Rainbow(constraint_func = constraint_func, **agent_params)\n",
    "    elif agent_type == 'SSBAS':\n",
    "        should_eliminate = False\n",
    "        agent = CSRL(BaseAgent = Rainbow, agent_params = agent_params, constraint_func_list=constraint_func_list,\n",
    "                     epsilon_decay_list=epsilon_decay_list, less_constrained_constraints_list=less_constrained_constraints_list, \n",
    "                     more_constrained_constraints_list=more_constrained_constraints_list, \n",
    "                     should_eliminate = should_eliminate, **csrl_params)\n",
    "    elif agent_type == 'CSRL':\n",
    "        should_eliminate = True\n",
    "        agent = CSRL(BaseAgent = Rainbow, agent_params = agent_params, constraint_func_list=constraint_func_list,\n",
    "                     epsilon_decay_list=epsilon_decay_list, less_constrained_constraints_list=less_constrained_constraints_list, \n",
    "                     more_constrained_constraints_list=more_constrained_constraints_list, \n",
    "                     should_eliminate = should_eliminate, **csrl_params)\n",
    "\n",
    "    run_experiment(env, agent, n_episodes = 5000, log_r = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142bfe9",
   "metadata": {},
   "source": [
    "## Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_type == 'edu':\n",
    "    \n",
    "    from environments_and_constraints.education.DKT_env import get_DKT_env\n",
    "    from environments_and_constraints.education.utils import *\n",
    "    \n",
    "    #Make the environment\n",
    "    env = get_DKT_env()\n",
    "    thresh = 0.85\n",
    "    action_size = env.n_actions\n",
    "    state_size = env.n_state_features\n",
    "    \n",
    "    #Make the constraints\n",
    "    saved_data_folder = 'environments_and_constraints/education/saved_data'\n",
    "    with open(f'{saved_data_folder}/problem_to_idx.json', 'r') as f:\n",
    "        problem_to_idx = convert_keys_to_int(json.load(f))\n",
    "    constraints_names = ['none', 'C8', 'C30', 'C50', 'C55', 'C60', 'C65', 'C70', 'C75', 'C80', 'C85', 'C90', 'C95', 'C100']\n",
    "    constraint_func_list, epsilon_decay_list, more_constrained_constraints_list, less_constrained_constraints_list = get_info(constraints_names, problem_to_idx, THRESH = thresh, n_actions = action_size)\n",
    "\n",
    "    agent_params = {\n",
    "        'state_size': state_size,\n",
    "        'action_size': action_size,\n",
    "        'hidden_units': [400, 400],\n",
    "        'gamma': 1,\n",
    "        'max_buffer_size': int(1e5),\n",
    "        'batch_size': 32,\n",
    "        'lr': 1e-4,\n",
    "        'copy_param': 200,\n",
    "        'double': True,\n",
    "        'epsilon_init': 1,\n",
    "        'decay_epsilon_experience': True\n",
    "    }\n",
    "    csrl_params = {\n",
    "        'constraint_conf_mult': 1,\n",
    "        'max_value': 50,\n",
    "        'offset': 0.0,\n",
    "        'policy_track_length': 20,\n",
    "        'loss_threshold': 0.1,\n",
    "    }\n",
    "    \n",
    "    agent_type = 'CSRL'\n",
    "    constraint_func = constraint_func_list[0]\n",
    "    epsilon_decay = epsilon_decay_list[0]\n",
    "    if agent_type == 'DQN':\n",
    "        agent_params['double'] = False\n",
    "        agent = DDQN_tf(constraint_func = constraint_func, epsilon_decay = epsilon_decay, **agent_params)\n",
    "    elif agent_type == 'DDQN':\n",
    "        agent_params['double'] = True\n",
    "        agent = DDQN_tf(constraint_func = constraint_func, epsilon_decay = epsilon_decay, **agent_params)\n",
    "    elif agent_type == 'SSBAS':\n",
    "        should_eliminate = False\n",
    "        agent = CSRL(BaseAgent = DDQN_tf, agent_params = agent_params, constraint_func_list=constraint_func_list,\n",
    "                     epsilon_decay_list=epsilon_decay_list, less_constrained_constraints_list=less_constrained_constraints_list, \n",
    "                     more_constrained_constraints_list=more_constrained_constraints_list, \n",
    "                     should_eliminate = should_eliminate, **csrl_params)\n",
    "    elif agent_type == 'CSRL':\n",
    "        should_eliminate = True\n",
    "        agent = CSRL(BaseAgent = DDQN_tf, agent_params = agent_params, constraint_func_list=constraint_func_list,\n",
    "                     epsilon_decay_list=epsilon_decay_list, less_constrained_constraints_list=less_constrained_constraints_list, \n",
    "                     more_constrained_constraints_list=more_constrained_constraints_list, \n",
    "                     should_eliminate = should_eliminate, **csrl_params)\n",
    "    \n",
    "    run_experiment(env, agent, n_episodes = 5000, log_r = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a71fd",
   "metadata": {},
   "source": [
    "## HIV Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_type == 'hiv':\n",
    "    \n",
    "    from environments_and_constraints.hiv.hiv_env import HIVTreatmentWrapper\n",
    "    from environments_and_constraints.hiv.utils import *\n",
    "    \n",
    "    #Make the environment\n",
    "    group = 2 #Sets the environment dynamices, can be: None, 0, 1, 2, 3, 4, or 5 (None, 2, and 5 were used)\n",
    "    with open('environments_and_constraints/hiv/hiv_preset_hidden_params', 'rb') as f:\n",
    "        preset_params_list = pickle.load(f, encoding=\"latin1\")\n",
    "    if group is None:\n",
    "        param_set = None\n",
    "    else:\n",
    "        param_set = preset_params_list[group]\n",
    "        \n",
    "    env = HIVTreatmentWrapper(param_set = param_set)\n",
    "    action_size = 4\n",
    "    state_size = 6\n",
    "    \n",
    "    #Create the constraints:\n",
    "    constraints_names = ['none', 'pNone_2', 'pNone_1', 'p2_1', 'p2_2', 'p4_1', 'p4_2']\n",
    "    constraint_func_list, epsilon_decay_list, more_constrained_constraints_list, less_constrained_constraints_list = get_info(constraints_names)\n",
    "    \n",
    "    agent_params = {\n",
    "        'state_size': state_size,\n",
    "        'action_size': action_size,\n",
    "        'hidden_units': [256, 512],\n",
    "        'gamma': 0.98,\n",
    "        'max_buffer_size': int(1e5),\n",
    "        'batch_size': 32,\n",
    "        'lr': 2.5e-4,\n",
    "        'copy_param': 200,\n",
    "        'double': True,\n",
    "        'epsilon_init': 1,\n",
    "        'decay_epsilon_experience': True\n",
    "    }\n",
    "    csrl_params = {\n",
    "        'constraint_conf_mult': 1,\n",
    "        'max_value': 10000,\n",
    "        'offset': 0.25,\n",
    "        'policy_track_length': 20,\n",
    "        'loss_threshold': 0.1,\n",
    "    }\n",
    "\n",
    "    constraint_func = constraint_func_list[0]\n",
    "    epsilon_decay = epsilon_decay_list[0]\n",
    "    \n",
    "    agent_type = 'CSRL'\n",
    "    if agent_type == 'DQN':\n",
    "        agent_params['double'] = False\n",
    "        agent = DDQN_tf(constraint_func = constraint_func, epsilon_decay = epsilon_decay, **agent_params)\n",
    "    elif agent_type == 'DDQN':\n",
    "        agent_params['double'] = True\n",
    "        agent = DDQN_tf(constraint_func = constraint_func, epsilon_decay = epsilon_decay, **agent_params)\n",
    "    elif agent_type == 'SSBAS':\n",
    "        should_eliminate = False\n",
    "        agent = CSRL(BaseAgent = DDQN_tf, agent_params = agent_params, constraint_func_list=constraint_func_list,\n",
    "                     epsilon_decay_list=epsilon_decay_list, less_constrained_constraints_list=less_constrained_constraints_list, \n",
    "                     more_constrained_constraints_list=more_constrained_constraints_list, \n",
    "                     should_eliminate = should_eliminate, **csrl_params)\n",
    "    elif agent_type == 'CSRL':\n",
    "        should_eliminate = True\n",
    "        agent = CSRL(BaseAgent = DDQN_tf, agent_params = agent_params, constraint_func_list=constraint_func_list,\n",
    "                     epsilon_decay_list=epsilon_decay_list, less_constrained_constraints_list=less_constrained_constraints_list, \n",
    "                     more_constrained_constraints_list=more_constrained_constraints_list, \n",
    "                     should_eliminate = should_eliminate, **csrl_params)\n",
    "    \n",
    "    run_experiment(env, agent, n_episodes = 5000, log_r = True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSRL",
   "language": "python",
   "name": "csrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
